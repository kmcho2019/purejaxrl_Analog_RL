{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2da97ce-0af1-4ea6-a349-974a606f186a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T06:44:53.964834Z",
     "iopub.status.busy": "2024-01-09T06:44:53.964598Z",
     "iopub.status.idle": "2024-01-09T06:44:53.971455Z",
     "shell.execute_reply": "2024-01-09T06:44:53.970438Z",
     "shell.execute_reply.started": "2024-01-09T06:44:53.964808Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Replace 0 with your GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75281cf7-fcbe-444b-bc03-3e15aaa1171b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T06:44:54.198069Z",
     "iopub.status.busy": "2024-01-09T06:44:54.197858Z",
     "iopub.status.idle": "2024-01-09T06:44:56.738746Z",
     "shell.execute_reply": "2024-01-09T06:44:56.737541Z",
     "shell.execute_reply.started": "2024-01-09T06:44:54.198050Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Re-implementation of Discovered Policy Optimisation (DPO)\n",
    "\n",
    "https://arxiv.org/abs/2210.05639\n",
    "\n",
    "This differs from PPO in just a few lines of the policy objective.\n",
    "\n",
    "Please refer to the paper for more details.\n",
    "\"\"\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "import gymnax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from wrappers import (\n",
    "    LogWrapper,\n",
    "    GymnaxWrapper,\n",
    "    VecEnv,\n",
    "    NormalizeVecObservation,\n",
    "    NormalizeVecReward,\n",
    "    ClipAction,\n",
    "    FlattenObservationWrapper\n",
    ")\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    #env = FlattenObservationWrapper(env)\n",
    "    #env = LogWrapper(env)\n",
    "    if config[\"NORMALIZE_ENV\"]:\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(\n",
    "            env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        alpha = config[\"DPO_ALPHA\"]\n",
    "                        beta = config[\"DPO_BETA\"]\n",
    "                        log_diff = log_prob - traj_batch.log_prob\n",
    "                        ratio = jnp.exp(log_diff)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        is_pos = (gae >= 0.0).astype(\"float32\")\n",
    "                        r1 = ratio - 1.0\n",
    "                        drift1 = nn.relu(r1 * gae - alpha * nn.tanh(r1 * gae / alpha))\n",
    "                        drift2 = nn.relu(\n",
    "                            log_diff * gae - beta * nn.tanh(log_diff * gae / beta)\n",
    "                        )\n",
    "                        drift = drift1 * is_pos + drift2 * (1 - is_pos)\n",
    "                        loss_actor = -(ratio * gae - drift).mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "\n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][\n",
    "                        info[\"returned_episode\"]\n",
    "                    ]\n",
    "                    timesteps = (\n",
    "                        info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    )\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(\n",
    "                            f\"global step={timesteps[t]}, episodic return={return_values[t]}\"\n",
    "                        )\n",
    "\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad17269-1a5a-46f9-866d-f4abe7cc8242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T06:44:56.740544Z",
     "iopub.status.busy": "2024-01-09T06:44:56.740179Z",
     "iopub.status.idle": "2024-01-09T06:45:01.035560Z",
     "shell.execute_reply": "2024-01-09T06:45:01.034026Z",
     "shell.execute_reply.started": "2024-01-09T06:44:56.740521Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './layers.0.weight.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3e-4\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_ENVS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m }\n\u001b[1;32m     22\u001b[0m rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m train_jit \u001b[38;5;241m=\u001b[39m \u001b[43mmake_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m train_jit(rng)\n",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m, in \u001b[0;36mmake_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     80\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_UPDATES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     81\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOTAL_TIMESTEPS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_STEPS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_ENVS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMINIBATCH_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_ENVS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_STEPS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_MINIBATCHES\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     85\u001b[0m )\n\u001b[0;32m---> 86\u001b[0m env, env_params \u001b[38;5;241m=\u001b[39m \u001b[43mgymnax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mENV_NAME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m env \u001b[38;5;241m=\u001b[39m LogWrapper(env)\n\u001b[1;32m     88\u001b[0m env \u001b[38;5;241m=\u001b[39m ClipAction(env)\n",
      "File \u001b[0;32m~/Analog_GPU_RL_SJPark_coop/Env/gymnax_Analog_RL/gymnax/registration.py:99\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(env_id, **env_kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 5. Custom environments\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m env_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTwoStageOTA-custom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mTwoStageOTA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment ID is not registered.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Analog_GPU_RL_SJPark_coop/Env/gymnax_Analog_RL/gymnax/environments/custom/two_stage_ota.py:104\u001b[0m, in \u001b[0;36mTwoStageOTA.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minit(key, dummy_input)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Load and update model parameters\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m new_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_model_params(new_params)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Assume that params remain static for the jit-ed functions\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# JIT compile the step_env method\u001b[39;00m\n",
      "File \u001b[0;32m~/Analog_GPU_RL_SJPark_coop/Env/gymnax_Analog_RL/gymnax/environments/custom/two_stage_ota.py:115\u001b[0m, in \u001b[0;36mTwoStageOTA.load_model_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    114\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_0\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./layers.0.weight.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.0.bias.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)},\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_1\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.2.weight.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.2.bias.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)},\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_2\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.4.weight.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.4.bias.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)},\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_3\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.6.weight.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.6.bias.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)},\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_4\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.8.weight.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./layers.8.bias.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n\u001b[1;32m    120\u001b[0m     }\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_params\n",
      "File \u001b[0;32m~/.conda/envs/Analog_jax/lib/python3.9/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './layers.0.weight.npy'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": 2048,\n",
    "    \"NUM_STEPS\": 10,\n",
    "    \"TOTAL_TIMESTEPS\": 5e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 32,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"DPO_ALPHA\": 2.0,\n",
    "    \"DPO_BETA\": 0.6,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ENV_NAME\": \"TwoStageOTA-custom\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "    \"NORMALIZE_ENV\": True,\n",
    "    \"DEBUG\": True,\n",
    "}\n",
    "rng = jax.random.PRNGKey(30)\n",
    "train_jit = make_train(config)\n",
    "out = train_jit(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6062813-36e4-4cff-9671-51bf9a8d3891",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-09T06:45:01.036466Z",
     "iopub.status.idle": "2024-01-09T06:45:01.036767Z",
     "shell.execute_reply": "2024-01-09T06:45:01.036643Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "rng = jax.random.PRNGKey(42)\n",
    "t0 = time.time()\n",
    "out = jax.block_until_ready(train_jit(rng))\n",
    "print(f\"time: {time.time() - t0:.2f} s\")\n",
    "plt.plot(out[\"metrics\"][\"returned_episode_returns\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369d32d-1fe8-490a-aa2a-1e0513dadd26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analog_jax",
   "language": "python",
   "name": "analog_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
